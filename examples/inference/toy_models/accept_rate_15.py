import numpy as np
import matplotlib.pyplot as plt
import sys
from scipy.stats import truncnorm, norm
from scipy import optimize

print("type(sys.argv[0]): ", type(sys.argv[0]))
print(sys.argv[1],",",sys.argv[2])
trial = int(sys.argv[1]) # 3000
accept = int(sys.argv[2]) # 100
if len(sys.argv)>3:
    dim = int(sys.argv[3]) # 15
else:
    dim = 15
std = 0.04
true_points = 0.1

def nnlf(params, data,lower,upper):
    # print("inside nnlf: data shape: ", data.shape)
    # print("inside nnlf: lower, upper ", lower, upper)
    loc, scale = params
    left_trunc_norm = (lower - loc)/scale
    right_trunc_norm = (upper - loc) / scale
    theta = (left_trunc_norm, right_trunc_norm, loc, scale)
    value = truncnorm.nnlf(theta, data)
    return value

#Simulate a prediction that would have come from data generated by theta and returns posterior max and posterior mean
def pred_func(theta):
    post_max = truncnorm.rvs(a=-theta/std, b=(1-theta)/std, loc=theta, scale=std, size=theta.shape)
    # loc = norm.rvs(loc=theta, scale=std, size=theta.shape)
    print("post max shape: ", post_max.shape)
    # if post_max.ndim == 1:
    #     post_max = np.expand_dims(post_max,axis=0)
    post_mean = np.array([truncnorm.mean(a=-post_max.T[i]/std, b=(1-post_max.T[i])/std, loc=post_max.T[i], scale=std) for i in range(post_max.shape[-1])]).T
    # post_mean = truncnorm.mean(a=-post_max/std, b=(1-post_max)/std, loc=post_max, scale=std)
    print("post mean shape: ", post_mean.shape)
    return np.array([post_max, post_mean])

def true_posterior(post_max):
    l = np.linspace(0,1,100)
    p = truncnorm.pdf(l,a=-post_max/std, b=(1-post_max)/std, loc=post_max, scale=std)
    return l,p

def p_value(post_max,true_param):
    p =1 - truncnorm.cdf(true_param + abs(true_param-post_max), a=-post_max / std, b=(1 - post_max) / std, loc=post_max, scale=std) +\
    truncnorm.cdf(true_param - abs(true_param-post_max), a=-post_max / std, b=(1 - post_max) / std, loc=post_max, scale=std)
    return p


f, ax = plt.subplots(2,2,figsize=(15,15))#,sharex=True,sharey=True)
f.suptitle('Accepted/Trial = ' + str(accept) + '/' + str(trial),fontsize=16)




true_param = np.ones(dim)*true_points
theta = np.random.rand(trial,dim)

# pred_theta = truncnorm.rvs(a=-theta/std, b=(1-theta)/std, loc=theta, scale=std, size=theta.shape)
pred_theta = pred_func(theta)[1]
# pred_param = truncnorm.rvs(a=-true_param/std, b=(1-true_param)/std, loc=true_param, scale=std, size=true_param.shape)
pred_param = pred_func(true_param)[1]
dist = np.linalg.norm(pred_param-pred_theta, axis=1)
accepted_ind = np.argpartition(dist,accept)[0:accept]
accepted_para = theta[accepted_ind]
accepted_mean = np.mean(accepted_para,0)
deviation = np.linalg.norm(accepted_mean[0:2] - true_param[0:2])
std_mean = np.mean(np.std(accepted_para, 0))
loc_opt = np.zeros(2)
scale_opt = np.zeros(2)
for j in range(2):
    loc_opt[j], scale_opt[j] = optimize.fmin(nnlf, (np.mean(accepted_para[:, j]), np.std(accepted_para[:, j])),
                                             args=(accepted_para[:, j], 0, 1), disp=False)
print("accepted_para shape: ", accepted_para.shape)
ax[0, 1].scatter(accepted_para[:,0],accepted_para[:,1], c='b', alpha=0.2, s=2)
ax[0, 1].scatter(true_param[0], true_param[1], marker='x', c='black',label='true param')
ax[0, 1].scatter(accepted_mean[0], accepted_mean[1], marker='x', c='orange',label='posterior mean' )
ax[0, 1].scatter(loc_opt[0], loc_opt[1], marker='x', c='red',label='posterior max')
ax[0, 1].scatter(pred_param[0], pred_param[1], marker='x', c='green', label='regression prediction')


ax[0, 1].plot([0,1,1,0,0], [0,0,1,1,0])
ax[0, 1].set_title(str(dim) + '-D, e: ' + '{0:.3f}'.format(deviation) + ', std: '
                            + '{0:.3f}'.format(std_mean))
ax[0,1].legend()


pval = p_value(pred_param[0],true_param[0])
ax[0, 0].set_title("p_val: " +  '{0:.3f}'.format(pval))
left_trunc_norm = (0 - loc_opt[0]) / scale_opt[0]
right_trunc_norm = (1 - loc_opt[0]) / scale_opt[0]
l = np.linspace(-0.1, 1.1, 1000)
p = truncnorm.pdf(l, left_trunc_norm, right_trunc_norm, loc_opt[0], scale_opt[0])
d=ax[0,0].hist(accepted_para[:,0], density=True, alpha = 0.2)
h=np.max(d[0])
ax[0,0].plot([true_param[0], true_param[0]],[h,0], c='black',label='true param')
ax[0,0].plot([loc_opt[0], loc_opt[0]], [h, 0], c='red',linestyle=':', label='post max')
ax[0,0].plot([pred_param[0], pred_param[0]], [h, 0], c='green',linestyle='--', label='regression prediction')
ltp,ptp = true_posterior(pred_param[0])
ptp = ptp/np.max(ptp)*np.max(p)

ax[0,0].plot(ltp,ptp, c='green', label='true posterior dist.(normalized)')
ax[0,0].plot([accepted_mean[0], accepted_mean[0]],[h,0], c='orange', label='posterior mean')
ax[0,0].plot(l, p, c='red')
ax[0,0].legend()


pval = p_value(pred_param[1],true_param[1])
ax[1, 1].set_title("p_val: " +  '{0:.3f}'.format(pval))
left_trunc_norm = (0 - loc_opt[1]) / scale_opt[1]
right_trunc_norm = (1 - loc_opt[1]) / scale_opt[1]
p = truncnorm.pdf(l, left_trunc_norm, right_trunc_norm, loc_opt[1], scale_opt[1])
d = ax[1, 1].hist(accepted_para[:, 1], alpha=0.2, density=True)
h = np.max(d[0])
ax[1, 1].plot([true_param[1], true_param[1]], [h, 0],c='black',label='true param')
ax[1, 1].plot([loc_opt[1], loc_opt[1]], [h, 0], c='red', linestyle=':', label='post max')
ax[1, 1].plot([pred_param[1], pred_param[1]], [h, 0], c='green',linestyle='--', label='regression prediction')
ltp,ptp = true_posterior(pred_param[1])
ptp = ptp/np.max(ptp)*np.max(p)
ax[1, 1].plot(ltp,ptp, c='green', label='true posterior dist.(normalized)')
ax[1, 1].plot([accepted_mean[1], accepted_mean[1]], [h, 0], c='orange', label='posterior mean')
ax[1, 1].plot(l, p, c='red')
ax[1, 1].legend()

plt.show()

