{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood-Free Parameter Inference on the Vilar Model\n",
    "\n",
    "This notebook illustrates neural network-based (henceforth referred to as ANN) inference and approximate Bayesian computation.\n",
    "The ANN models learn the relationship ${\\bf y} \\rightarrow {\\bf \\theta}$, where ${\\bf y}$ is a time series response and ${\\bf \\theta}$ are the parameters of the descriptive model.\n",
    "Therefore, we require a training set of the form $({\\bf y, \\theta})$ to train the ANN models. We will first generate such a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import dask\n",
    "import numpy as np\n",
    "import gillespy2\n",
    "from gillespy2.solvers.cpp import SSACSolver\n",
    "from sciope.utilities.priors import uniform_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function sets specified values of model parameters\n",
    "def set_model_parameters(model, params):\n",
    "    \"\"\" params - array, needs to have the same order as\n",
    "        model.listOfParameters \"\"\"\n",
    "    model.tspan = np.linspace(1, 100, 100)\n",
    "    for e, (pname, p) in enumerate(model.listOfParameters.items()):\n",
    "        model.get_parameter(pname).set_expression(params[e])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_parameters(model, params):\n",
    "    params = params.ravel()\n",
    "    model.tspan = np.linspace(1, 100, 100)\n",
    "    \n",
    "    temp_param = model.get_parameter('alpha_A')\n",
    "    temp_param.set_expression(params[0])\n",
    "    \n",
    "    temp_param = model.get_parameter('alpha_a_prime')\n",
    "    temp_param.set_expression(params[1])\n",
    "    \n",
    "    temp_param = model.get_parameter('alpha_r')\n",
    "    temp_param.set_expression(params[2])\n",
    "    \n",
    "    temp_param = model.get_parameter('alpha_r_prime')\n",
    "    temp_param.set_expression(params[3])\n",
    "    \n",
    "    temp_param = model.get_parameter('beta_a')\n",
    "    temp_param.set_expression(params[4])\n",
    "    \n",
    "    temp_param = model.get_parameter('beta_r')\n",
    "    temp_param.set_expression(params[5])\n",
    "    \n",
    "    temp_param = model.get_parameter('delta_ma')\n",
    "    temp_param.set_expression(params[6])\n",
    "    \n",
    "    temp_param = model.get_parameter('delta_mr')\n",
    "    temp_param.set_expression(params[7])\n",
    "    \n",
    "    temp_param = model.get_parameter('delta_a')\n",
    "    temp_param.set_expression(params[8])\n",
    "    \n",
    "    temp_param = model.get_parameter('delta_r')\n",
    "    temp_param.set_expression(params[9])\n",
    "    \n",
    "    temp_param = model.get_parameter('gamma_a')\n",
    "    temp_param.set_expression(params[10])\n",
    "    \n",
    "    temp_param = model.get_parameter('gamma_r')\n",
    "    temp_param.set_expression(params[11])\n",
    "    \n",
    "    temp_param = model.get_parameter('gamma_c')\n",
    "    temp_param.set_expression(params[12])\n",
    "    \n",
    "    temp_param = model.get_parameter('Theta_a')\n",
    "    temp_param.set_expression(params[13])\n",
    "    \n",
    "    temp_param = model.get_parameter('Theta_r')\n",
    "    temp_param.set_expression(params[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the model definition from a file and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc = gillespy2.StochMLDocument.from_file(\"/home/ubuntu/code/sciope/examples/inference/vilar/StochSS_model/vilar_oscillator_AIYDNg/models/data/vilar_oscillator.xml\")\n",
    "model = model_doc.to_model(\"Vilar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a search prior for the parameter inference problem, and our 'true' parameter point that corresponds to observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin = [0,    100,    0,   20,   10,   1,    1,   0,   0,   0, 0.5,    0,   0,    0,   0]\n",
    "dmax = [80,   600,    4,   60,   60,   7,   12,   2,   3, 0.7, 2.5,   4,   3,   70,   300]\n",
    "v_prior = uniform_prior.UniformPrior(np.asarray(dmin), np.asarray(dmax))\n",
    "fixed_point = np.asarray([50.0, 500.0, 0.01, 50.0, 50.0, 5.0, 10.0, 0.5, 1.0, 0.2, 1.0, 1.0, 2.0, 50.0, 100.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to generate some samples based on the prior above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "samples_delayed = v_prior.draw(N)\n",
    "samples ,= dask.compute(samples_delayed)\n",
    "samples = np.asarray(samples)\n",
    "samples = samples.reshape(N, len(dmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the simulator as below. We will concentrate on the species C,A,R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trajs = 50\n",
    "num_timestamps = 101\n",
    "num_species = 3\n",
    "\n",
    "def sim_vilar(param, trajs=1):\n",
    "    num_trajs = trajs\n",
    "    model = model_doc.to_model(\"Vilar\")\n",
    "    set_model_parameters(model, param)\n",
    "    sim_results = model.run(solver=SSACSolver, show_labels=False, number_of_trajectories=num_trajs)\n",
    "    tot_res = np.asarray([x.T for x in sim_results]) # reshape to (N, S, T)  \n",
    "    tot_res = tot_res[:,1:, :] # should not contain timepoints\n",
    "    tot_res = tot_res[:,6:9,:].reshape((num_trajs,num_species,num_timestamps))\n",
    "    return tot_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate the samples from the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for i in range(N):\n",
    "    current_sample = samples[i,:].reshape(1, len(samples[i,:]))\n",
    "    \n",
    "    lazy_response = dask.delayed(sim_vilar)(current_sample)\n",
    "    responses.append(lazy_response)\n",
    "\n",
    "# dask compute\n",
    "computed_responses ,= dask.compute(responses)\n",
    "\n",
    "# get it in the right shape sciope shape - N x S x T\n",
    "ts = np.asarray(computed_responses)\n",
    "ts = ts.reshape(N, num_species, num_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data, we can train the ANN models to learn the mapping from ${\\bf y}$ to ${\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output shape for the CNN\n",
    "input_shape = (101,3)\n",
    "output_shape = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routines to normalize and denormalize data\n",
    "# Makes training easier\n",
    "def normalize_data(data, dmin, dmax):\n",
    "    dmin = np.array(dmin)\n",
    "    dmax = np.array(dmax)\n",
    "    return (data - dmin)/(dmax-dmin)\n",
    "\n",
    "def denormalize_data(data, dmin, dmax):\n",
    "    dmin = np.array(dmin)\n",
    "    dmax = np.array(dmax)\n",
    "    denorm = data * (dmax-dmin) + dmin\n",
    "    return denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3, 101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = np.asarray(computed_responses)\n",
    "ts = ts.reshape(N, num_species, num_timestamps)\n",
    "ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_thetas = normalize_data(samples, dmin, dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CNN model\n",
    "from sciope.models.cnn_regressor import CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model_cnn = CNNModel(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = model_cnn.train(ts, normed_thetas, batch_size=256, \n",
    "                      epochs=500, verbose=0, learning_rate=0.001, \n",
    "                      early_stopping_patience=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate the models to see how accurate they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 10000\n",
    "samples_delayed = v_prior.draw(N_test)\n",
    "samples_test ,= dask.compute(samples_delayed)\n",
    "samples_test = np.asarray(samples_test)\n",
    "samples_test = samples_test.reshape(N_test, len(dmin))\n",
    "\n",
    "responses_test = []\n",
    "\n",
    "for i in range(N_test):\n",
    "    current_sample = samples_test[i,:].reshape(1, len(samples_test[i,:]))\n",
    "    \n",
    "    lazy_response = dask.delayed(sim_vilar)(current_sample)\n",
    "    responses_test.append(lazy_response)\n",
    "\n",
    "# dask compute\n",
    "computed_responses_test ,= dask.compute(responses_test)\n",
    "\n",
    "# get it in the right shape sciope shape - N x S x T\n",
    "ts_test = np.asarray(computed_responses_test)\n",
    "ts_test = ts_test.reshape(N_test, num_species, num_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'observed' data\n",
    "response_obs = sim_vilar(fixed_point, trajs=1)\n",
    "ts_obs = response_obs.reshape(1, num_species, num_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to test each ANN architecture on MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def test_model(model):\n",
    "    pred_test = model.predict(ts_test)\n",
    "    pred_test = denormalize_data(pred_test, dmin, dmax)\n",
    "    #mae_test = np.mean(abs(pred_test - samples_test), axis=0)\n",
    "    mae_test = mean_absolute_error(samples_test, pred_test)\n",
    "    \n",
    "    theta_pred = model.predict(ts_obs)\n",
    "    samples_true = np.asarray(fixed_point)\n",
    "    theta_pred = denormalize_data(theta_pred, dmin, dmax)\n",
    "    mae_true = np.mean(np.abs(theta_pred.ravel() - samples_true), axis=0)\n",
    "    #mae_true = mean_absolute_error(np.asarray(samples_true).reshape(1,2), np.asarray(theta_pred).reshape(1,2))\n",
    "    return mae_test, mae_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics\n",
    "mae_test_cnn, mae_true_cnn = test_model(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test MAE = 16.271204419741156, MAE at true point = 12.902529955863953\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test MAE = {}, MAE at true point = {}\".format(mae_test_cnn, mae_true_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciope.utilities.summarystats.summary_base import SummaryBase\n",
    "from sciope.utilities.housekeeping import sciope_logger as ml\n",
    "\n",
    "class ANN_Statistics(SummaryBase):\n",
    "    \"\"\"\n",
    "    The thetas predicted by ANN models act as summary statistics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean_trajectories=False, use_logger=False):\n",
    "        self.name = 'ANN_Statistics'\n",
    "        super(ANN_Statistics, self).__init__(self.name, mean_trajectories, use_logger)\n",
    "        if self.use_logger:\n",
    "            self.logger = ml.SciopeLogger().get_logger()\n",
    "            self.logger.info(\"ANN_Statistics summary statistic initialized\")\n",
    "\n",
    "    def compute(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the value(s) of the summary statistic(s)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : [type]\n",
    "            simulated or data set in the form N x S X T - num data points x num species x num time steps\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        [type]\n",
    "            computed statistic value\n",
    "        \n",
    "        \"\"\"\n",
    "        data_arr = np.array(data)\n",
    "        assert len(data_arr.shape) == 3, \"required input shape is (n_points, n_species, n_timepoints)\"\n",
    "\n",
    "        res = model_cnn.predict(data_arr)\n",
    "        res = denormalize_data(res, dmin, dmax)\n",
    "\n",
    "        if self.mean_trajectories:\n",
    "            res = np.asarray(np.mean(res, axis=0))  # returns a scalar, so we cast it as an array\n",
    "\n",
    "        if self.use_logger:\n",
    "            self.logger.info(\"ANN_Statistics summary statistic: processed data matrix of shape {0} and generated summaries\"\n",
    "                             \" of shape {1}\".format(data.shape, res.shape))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our new summary statistic\n",
    "cnn_stat = ANN_Statistics()\n",
    "predicted_stat = cnn_stat.compute(ts_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.46384788e+01 3.83292413e+02 1.97799933e+00 4.12151909e+01\n",
      "  3.92695999e+01 4.63687801e+00 5.90746319e+00 6.15483046e-01\n",
      "  1.13809007e+00 2.92096889e-01 1.37776500e+00 2.12254214e+00\n",
      "  1.85050839e+00 4.89150506e+01 1.32449555e+02]]\n",
      "MAE upon comparison as a statistic = 12.902529955863953\n"
     ]
    }
   ],
   "source": [
    "print(predicted_stat)\n",
    "stat_mae = np.mean(np.abs(predicted_stat.ravel() - np.asarray(fixed_point)), axis=0)\n",
    "print(\"MAE upon comparison as a statistic = {}\".format(stat_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
