{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# Interactive backend required for model exploration\n",
    "import gillespy2\n",
    "from gillespy2.solvers.numpy import NumPySSASolver \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Gillespy2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToggleSwitch(gillespy2.Model):\n",
    "    \"\"\" Gardner et al. Nature (1999)\n",
    "    'Construction of a genetic toggle switch in Escherichia coli'\n",
    "    \"\"\"\n",
    "    def __init__(self, parameter_values=None):\n",
    "        # Initialize the model.\n",
    "        gillespy2.Model.__init__(self, name=\"toggle_switch\")\n",
    "        # Parameters\n",
    "        alpha1 = gillespy2.Parameter(name='alpha1', expression=1)\n",
    "        alpha2 = gillespy2.Parameter(name='alpha2', expression=1)\n",
    "        beta = gillespy2.Parameter(name='beta', expression=\"2.0\")\n",
    "        gamma = gillespy2.Parameter(name='gamma', expression=\"2.0\")\n",
    "        mu = gillespy2.Parameter(name='mu', expression=1.0)\n",
    "        self.add_parameter([alpha1, alpha2, beta, gamma, mu])\n",
    "\n",
    "        # Species\n",
    "        U = gillespy2.Species(name='U', initial_value=10)\n",
    "        V = gillespy2.Species(name='V', initial_value=10)\n",
    "        self.add_species([U, V])\n",
    "\n",
    "        # Reactions\n",
    "        cu = gillespy2.Reaction(name=\"r1\",reactants={}, products={U:1},\n",
    "                propensity_function=\"alpha1/(1+pow(V,beta))\")\n",
    "        cv = gillespy2.Reaction(name=\"r2\",reactants={}, products={V:1},\n",
    "                propensity_function=\"alpha2/(1+pow(U,gamma))\")\n",
    "        du = gillespy2.Reaction(name=\"r3\",reactants={U:1}, products={},\n",
    "                rate=mu)\n",
    "        dv = gillespy2.Reaction(name=\"r4\",reactants={V:1}, products={},\n",
    "                rate=mu)\n",
    "        self.add_reaction([cu,cv,du,dv])\n",
    "        self.timespan(np.linspace(0,50,101))\n",
    "\n",
    "toggle_model = ToggleSwitch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Sciope's Gillespy2 wrapper to extract simulator and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciope.utilities.gillespy2 import wrapper\n",
    "\n",
    "settings = {\"solver\": NumPySSASolver, \"number_of_trajectories\":10, \"show_labels\":True}\n",
    "simulator = wrapper.get_simulator(gillespy_model=toggle_model, run_settings=settings, species_of_interest=[\"U\", \"V\"])\n",
    "\n",
    "expression_array = wrapper.get_parameter_expression_array(toggle_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LHD for sampling and TSFRESH minimal summary statistics\n",
    "To generate a LHD for exploration we need to start a dask client. Points will be generated using distributed resources and persited at worker nodes,\n",
    "we can then draw random samples from the LHD to local RAM. In this example we use a local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from sciope.designs import latin_hypercube_sampling\n",
    "from sciope.utilities.summarystats.auto_tsfresh import SummariesTSFRESH\n",
    "\n",
    "c = Client()\n",
    "lhc = latin_hypercube_sampling.LatinHypercube(xmin=expression_array, xmax=expression_array*3)\n",
    "lhc.generate_array(1000) #creates a LHD of size 1000\n",
    "\n",
    "#will use default minimal set of features\n",
    "summary_stats = SummariesTSFRESH()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start local cluster using dask client and Model exploration with StochMET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciope.stochmet.stochmet import StochMET\n",
    "\n",
    "met = StochMET(simulator, lhc, summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run parameter sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.compute(n_points=500, chunk_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the result\n",
    "Here we will explore parameter points expressed in feature space using a dimension reduction method. User can interact with points and label points according to different model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets add some appropiate information about the model and features\n",
    "met.data.configurations['listOfParameters'] = list(toggle_model.listOfParameters.keys())\n",
    "met.data.configurations['listOfSpecies'] = list(toggle_model.listOfSpecies.keys())\n",
    "met.data.configurations['listOfSummaries'] = met.summaries.features\n",
    "met.data.configurations['timepoints'] = toggle_model.tspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we use UMAP for dimension reduction\n",
    "met.explore(dr_method='umap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once a few points has been added we can use Semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sciope.models.label_propagation import LPModel\n",
    "#here lets use the dimension reduction embedding as input data\n",
    "data = met.dr_model.embedding_\n",
    "\n",
    "model_lp = LPModel()\n",
    "#train using basinhopping\n",
    "model_lp.train(data, met.data.user_labels, min_=0.01, max_=10, niter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to vislualize the result we will map the label distribution to the user_labels (will enable us to see the LP model \n",
    "# output when using method \"explore\")\n",
    "\n",
    "user_labels = np.copy(met.data.user_labels)\n",
    "#takes the label corresponding to index 0\n",
    "met.data.user_labels = model_lp.model.label_distributions_[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "met.explore(dr_method='umap', from_distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met.data.user_labels = user_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciope",
   "language": "python",
   "name": "sciope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
